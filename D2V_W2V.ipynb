{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "D2V W2V.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyM0EPP9naBCvybuBR04UOr4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rushikeshnaik779/classification_algorithms_miscellaneous/blob/main/D2V_W2V.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rGbLYutrecBB",
        "outputId": "51f20471-e10e-425f-a495-e28667b47d7b"
      },
      "source": [
        "import pandas as pd\n",
        "import nltk \n",
        "nltk.download('stopwords')\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.model_selection import train_test_split\n",
        "from gensim.models.doc2vec import Doc2Vec, TaggedDocument"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qFjHjYylYmB3",
        "outputId": "67419935-4cf4-4ef6-b90a-f2ec6b7fcb64"
      },
      "source": [
        "#downloding data\n",
        "!wget -P DATAPATH https://raw.githubusercontent.com/practical-nlp/practical-nlp/master/Ch4/Data/Sentiment%20and%20Emotion%20in%20Text/train_data.csv\n",
        "!wget -P DATAPATH https://raw.githubusercontent.com/practical-nlp/practical-nlp/master/Ch4/Data/Sentiment%20and%20Emotion%20in%20Text/test_data.csv\n",
        "!ls -lah DATAPATH"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-02-04 10:29:15--  https://raw.githubusercontent.com/practical-nlp/practical-nlp/master/Ch4/Data/Sentiment%20and%20Emotion%20in%20Text/train_data.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2479133 (2.4M) [text/plain]\n",
            "Saving to: ‘DATAPATH/train_data.csv’\n",
            "\n",
            "train_data.csv      100%[===================>]   2.36M  --.-KB/s    in 0.1s    \n",
            "\n",
            "2021-02-04 10:29:15 (20.0 MB/s) - ‘DATAPATH/train_data.csv’ saved [2479133/2479133]\n",
            "\n",
            "--2021-02-04 10:29:15--  https://raw.githubusercontent.com/practical-nlp/practical-nlp/master/Ch4/Data/Sentiment%20and%20Emotion%20in%20Text/test_data.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 783640 (765K) [text/plain]\n",
            "Saving to: ‘DATAPATH/test_data.csv’\n",
            "\n",
            "test_data.csv       100%[===================>] 765.27K  --.-KB/s    in 0.03s   \n",
            "\n",
            "2021-02-04 10:29:16 (23.0 MB/s) - ‘DATAPATH/test_data.csv’ saved [783640/783640]\n",
            "\n",
            "total 3.2M\n",
            "drwxr-xr-x 2 root root 4.0K Feb  4 10:29 .\n",
            "drwxr-xr-x 1 root root 4.0K Feb  4 10:29 ..\n",
            "-rw-r--r-- 1 root root 766K Feb  4 10:29 test_data.csv\n",
            "-rw-r--r-- 1 root root 2.4M Feb  4 10:29 train_data.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "id": "INGXTf5OY2xI",
        "outputId": "dd2e3559-a3c1-472a-cad2-c0b4a67893dd"
      },
      "source": [
        "# load the dataset and explore \n",
        "filepath = \"DATAPATH/train_data.csv\"\n",
        "\n",
        "df = pd.read_csv(filepath)\n",
        "print(df.shape)\n",
        "df.head()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(30000, 2)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentiment</th>\n",
              "      <th>content</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>empty</td>\n",
              "      <td>@tiffanylue i know  i was listenin to bad habi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>sadness</td>\n",
              "      <td>Layin n bed with a headache  ughhhh...waitin o...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>sadness</td>\n",
              "      <td>Funeral ceremony...gloomy friday...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>enthusiasm</td>\n",
              "      <td>wants to hang out with friends SOON!</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>neutral</td>\n",
              "      <td>@dannycastillo We want to trade with someone w...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    sentiment                                            content\n",
              "0       empty  @tiffanylue i know  i was listenin to bad habi...\n",
              "1     sadness  Layin n bed with a headache  ughhhh...waitin o...\n",
              "2     sadness                Funeral ceremony...gloomy friday...\n",
              "3  enthusiasm               wants to hang out with friends SOON!\n",
              "4     neutral  @dannycastillo We want to trade with someone w..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rwnm4k1xZGn3",
        "outputId": "3d4bedba-31cd-46c0-c659-efcdbd7acf91"
      },
      "source": [
        "df['sentiment'].value_counts()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "worry         7433\n",
              "neutral       6340\n",
              "sadness       4828\n",
              "happiness     2986\n",
              "love          2068\n",
              "surprise      1613\n",
              "hate          1187\n",
              "fun           1088\n",
              "relief        1021\n",
              "empty          659\n",
              "enthusiasm     522\n",
              "boredom        157\n",
              "anger           98\n",
              "Name: sentiment, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b2Y9c8fQZL2r",
        "outputId": "aecf294e-320f-4eda-e6c2-03416e75692f"
      },
      "source": [
        "# let us take the top 3 categories and leave out the rest \n",
        "shortlist = ['neutral', 'happiness', 'worry']\n",
        "df_subset = df[df['sentiment'].isin(shortlist)]\n",
        "df_subset.shape"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(16759, 2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "byNYQgSBZqzA"
      },
      "source": [
        "## Text pre-processing:\n",
        "\n",
        "Tweets are different. Somethings to consider:\n",
        "- Removing @mentions and urls\n",
        "- using NLTK Tweet tokenizer instead of a regular one \n",
        "- stopwords, numbers as usual."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "spIzK06aZil4"
      },
      "source": [
        "tweeter = TweetTokenizer(strip_handles=True, preserve_case=False)\n",
        "mystopwords = set(stopwords.words(\"english\"))\n",
        "\n",
        "\n",
        "def preprocess_corpus(texts):\n",
        "    def remove_stops_digit(tokens):\n",
        "        return [token for token in tokens if token not in mystopwords and not token.isdigit()]\n",
        "    \n",
        "    return [remove_stops_digit(tweeter.tokenize(content)) for content in texts]"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J-C2cE9xa4X3",
        "outputId": "3f3f0f1c-79b6-40db-b15e-d829f5b7658f"
      },
      "source": [
        "mydata = preprocess_corpus(df_subset['content'])\n",
        "mycats = df_subset['sentiment']\n",
        "print(len(mydata), len(mycats))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "16759 16759\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uQNHYgV4bGRE",
        "outputId": "72637003-f165-4352-df07-55cbcd3327df"
      },
      "source": [
        "# spliting the data into train and test, following the usual process \n",
        "X_train , X_test, y_train, y_test = train_test_split(mydata, mycats, random_state=100)\n",
        "\n",
        "train_doc2vec = [TaggedDocument((d), tags=[str(i)]) for i, d in enumerate(X_train)]\n",
        "model = Doc2Vec(vector_size=50, alpha=0.025, min_count=5, dm=1, epochs=100)\n",
        "model.build_vocab(train_doc2vec)\n",
        "model.train(train_doc2vec, total_examples=model.corpus_count, epochs=model.epochs)\n",
        "model.save(\"d2v.model\")\n",
        "print(\"Model saved\")"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model saved\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9RX-A6-FciDM"
      },
      "source": [
        "model = Doc2Vec.load('d2v.model')\n",
        "\n",
        "train_vectors = [model.infer_vector(list_of_tokens, steps=50) for list_of_tokens in X_train]\n",
        "test_vectors = [model.infer_vector(list_of_tokens, steps=50) for list_of_tokens in X_test]\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g0t7ZNgXd77T"
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression \n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "\n",
        "myclass = LogisticRegression(class_weight=\"balanced\")\n",
        "myclass.fit(train_vectors, y_train)\n",
        "\n",
        "preds = myclass.predict(test_vectors)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UbCJZj09eZ68",
        "outputId": "171d2afb-5bad-4276-db64-a232b49742ba"
      },
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "print(classification_report(y_test, preds))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "   happiness       0.37      0.53      0.43       741\n",
            "     neutral       0.47      0.54      0.50      1567\n",
            "       worry       0.59      0.41      0.49      1882\n",
            "\n",
            "    accuracy                           0.48      4190\n",
            "   macro avg       0.48      0.50      0.48      4190\n",
            "weighted avg       0.51      0.48      0.48      4190\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MqKJ-d9yeyjX"
      },
      "source": [
        ""
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yuf2nycegMwg"
      },
      "source": [
        "WORD2VEC\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IdaffTmQgPDe",
        "outputId": "31aef761-d375-482b-9923-e5ec5439b260"
      },
      "source": [
        "!wget -P DATAPATH -c \"https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\""
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-02-04 10:32:41--  https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\n",
            "Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.217.71.78\n",
            "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.217.71.78|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1647046227 (1.5G) [application/x-gzip]\n",
            "Saving to: ‘DATAPATH/GoogleNews-vectors-negative300.bin.gz’\n",
            "\n",
            "GoogleNews-vectors- 100%[===================>]   1.53G  44.9MB/s    in 35s     \n",
            "\n",
            "2021-02-04 10:33:16 (45.2 MB/s) - ‘DATAPATH/GoogleNews-vectors-negative300.bin.gz’ saved [1647046227/1647046227]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "368OVfOiggEs"
      },
      "source": [
        "import os \n",
        "from time import time \n",
        "\n",
        "# preprocessing imports\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from string import punctuation\n",
        "\n",
        "\n",
        "# imports related to modeling \n",
        "import numpy as np \n",
        "from gensim.models import Word2Vec, keyedvectors, KeyedVectors\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m_YgyD7ij_0M"
      },
      "source": [
        "! cat DATAPATH/amazon_cells_labelled.txt DATAPATH/imdb_labelled.txt DATAPATH/yelp_labelled.txt > sentiment_sentences.txt"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9x7Zimf4harC"
      },
      "source": [
        "data_path = \"DATAPATH\"\n",
        "path_to_model = os.path.join(data_path, 'GoogleNews-vectors-negative300.bin.gz')\n"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uPmhYs1Fkn-e",
        "outputId": "efe076e3-aa4a-4b4b-d44d-bf0a3067a21e"
      },
      "source": [
        "# load w2v model this will take some time \n",
        "%time w2v_model = KeyedVectors.load_word2vec_format(path_to_model, binary=True)\n",
        "print(\"done loading word2vec\")"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 1min 45s, sys: 2.03 s, total: 1min 47s\n",
            "Wall time: 1min 47s\n",
            "done loading word2vec\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "auomlKPKlKDF"
      },
      "source": [
        "texts =[]\n",
        "cats = []\n",
        "fh = open(\"sentiment_sentences.txt\")\n",
        "for line in fh : \n",
        "    text, sentiment = line.split(\"\\t\")\n",
        "    texts.append(text)\n",
        "    cats.append(sentiment)\n"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yxQchG1qpNUv",
        "outputId": "84ae810b-015e-4ffd-9561-4bab76dd51a0"
      },
      "source": [
        "word2vec_vocab = w2v_model.vocab.keys()\n",
        "word2vec_vocab_lower = [item.lower() for item in word2vec_vocab]\n",
        "print(len(word2vec_vocab))"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3000000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JYWWyeDRtL9V",
        "outputId": "0146860b-ae8f-4f61-8a60-fdd30edea444"
      },
      "source": [
        "print(len(cats), len(texts))\n",
        "print(texts[1])\n",
        "print(cats[1])"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3000 3000\n",
            "Good case, Excellent value.\n",
            "1\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o4CFHGy0zKJN",
        "outputId": "7b5d917f-a1bc-448f-9b9f-90a222a62489"
      },
      "source": [
        "# preprocess the text \n",
        "nltk.download('punkt')\n",
        "def preprocess_corpus(texts):\n",
        "    mystopwords = set(stopwords.words(\"english\"))\n",
        "\n",
        "    def remove_stops_digit(tokens):\n",
        "        return [token.lower() for token in tokens if token not in mystopwords \n",
        "                and not token.isdigit() and token not in punctuation]\n",
        "\n",
        "    return [remove_stops_digit(word_tokenize(text)) for text in texts]\n",
        "\n",
        "\n",
        "texts_processed = preprocess_corpus(texts)\n",
        "print(len(cats), len(texts_processed))\n",
        "print(texts_processed[1])\n",
        "print(cats[1])"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "3000 3000\n",
            "['good', 'case', 'excellent', 'value']\n",
            "1\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "slXCVafSteEF"
      },
      "source": [
        "# creating a feature vector by averaging all embeddings for all sentences \n",
        "def embedding_feats(list_of_lists):\n",
        "    DIMENSION = 300\n",
        "    zero_vector = np.zeros(DIMENSION)\n",
        "    feats = []\n",
        "    for tokens in list_of_lists: \n",
        "        feat_for_this = np.zeros(DIMENSION)\n",
        "        count_for_this = 0 \n",
        "\n",
        "        for token in tokens: \n",
        "            if token in w2v_model:\n",
        "                feat_for_this += w2v_model[token]\n",
        "                count_for_this += 1\n",
        "        feats.append(feat_for_this/count_for_this)\n",
        "    \n",
        "    return feats"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bmcv7V8hyyFB",
        "outputId": "0977b7ee-b9bd-42e8-b8f9-c943a2c6eb68"
      },
      "source": [
        "train_vectors = embedding_feats(texts_processed)\n",
        "print(len(train_vectors))"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:14: RuntimeWarning: invalid value encountered in true_divide\n",
            "  \n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 358
        },
        "id": "PYUHD5jhy-Nk",
        "outputId": "15372ac8-9c7b-4a3c-a506-26525095efe1"
      },
      "source": [
        ""
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-38-d8b12a7cbca8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mclassifier\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGradientBoostingClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_vectors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcats\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Accuracy\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/ensemble/_gb.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, monitor)\u001b[0m\n\u001b[1;32m   1439\u001b[0m         \u001b[0;31m# Since check_array converts both X and y to the same dtype, but the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1440\u001b[0m         \u001b[0;31m# trees use different types for X and y, checking them separately.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1441\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'csr'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'csc'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'coo'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDTYPE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1442\u001b[0m         \u001b[0mn_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_features_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1443\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    576\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    577\u001b[0m             _assert_all_finite(array,\n\u001b[0;32m--> 578\u001b[0;31m                                allow_nan=force_all_finite == 'allow-nan')\n\u001b[0m\u001b[1;32m    579\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    580\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mensure_min_samples\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m_assert_all_finite\u001b[0;34m(X, allow_nan, msg_dtype)\u001b[0m\n\u001b[1;32m     58\u001b[0m                     \u001b[0mmsg_err\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m                     (type_err,\n\u001b[0;32m---> 60\u001b[0;31m                      msg_dtype if msg_dtype is not None else X.dtype)\n\u001b[0m\u001b[1;32m     61\u001b[0m             )\n\u001b[1;32m     62\u001b[0m     \u001b[0;31m# for object dtype data, we only check for NaNs (GH-13254)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Input contains NaN, infinity or a value too large for dtype('float32')."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 164
        },
        "id": "dNlW3HFE3m_L",
        "outputId": "483d776d-d3df-49b1-8304-3e974a9cf33e"
      },
      "source": [
        "train_vectors = train_vectors.fillna(train_vectors.mean())"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-43-b1b77cfa8afa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_vectors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_vectors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfillna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_vectors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'fillna'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oNmesz2l4NXb"
      },
      "source": [
        "train_vectors = pd.DataFrame(train_vectors)"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zo6o10TK4REY"
      },
      "source": [
        "train_vectors['cats'] = cats"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bSkWifD54ZHq"
      },
      "source": [
        "train_vectors = train_vectors.fillna(train_vectors.mean())\n",
        "\n"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L38Yon0W4iJq",
        "outputId": "acbd2bf6-30e4-4d58-ce85-c4191559da06"
      },
      "source": [
        "# taking logistic regression \n",
        "from sklearn.ensemble import RandomForestClassifier,AdaBoostClassifier,GradientBoostingClassifier\n",
        "classifier = LogisticRegression()\n",
        "X_train, X_test, y_train, y_test = train_test_split(train_vectors.drop(['cats'], axis=1), train_vectors['cats'])\n",
        "classifier.fit(X_train, y_train)\n",
        "print(\"Accuracy\", classifier.score(X_test, y_test))"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy 0.8386666666666667\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nIBXtRTV4wt5",
        "outputId": "ea9e3805-e4ab-4b9e-9df0-59594f31f59b"
      },
      "source": [
        "preds = classifier.predict(X_test)\n",
        "print(classification_report(y_test, preds))"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "          0\n",
            "       0.84      0.85      0.85       391\n",
            "          1\n",
            "       0.83      0.83      0.83       359\n",
            "\n",
            "    accuracy                           0.84       750\n",
            "   macro avg       0.84      0.84      0.84       750\n",
            "weighted avg       0.84      0.84      0.84       750\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H7KdEbgr5pX3"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}